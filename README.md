# CAG Chat System with Streamlit and Gemini

This project implements a chat application that allows users to upload documents (.txt, .pdf, .docx) and ask questions about their content. It utilizes a Context-Augmented Generation (CAG) approach by loading the entire document content into the context window of a Google Gemini language model.

## Features

*   **Document Upload:** Supports `.txt`, `.pdf`, and `.docx` file formats.
*   **CAG Approach:** Loads the full text content of the uploaded document into the LLM's context for each query.
*   **LLM Integration:** Uses the `models/gemini-2.5-pro-exp-03-25` model via the `google-generativeai` library.
*   **Token Counting:** Calculates and displays the token count of the uploaded document relative to the model's context limit (1,000,000 tokens).
*   **Context Limit Handling:** Prevents processing documents that exceed the token limit and warns the user.
*   **Conversational History:** Includes recent chat history (up to 4 pairs) in the prompt sent to the LLM for better conversational flow.
*   **Dynamic History Truncation:** Attempts to reduce chat history included in the prompt if the combined size (history + document + query) exceeds the token limit.
*   **Streamlit UI:** Provides a web-based interface for file upload and chat interaction.

## Setup

1.  **Prerequisites:**
    *   Python 3.8+ recommended.
    *   `pip` and `venv` (usually included with Python).

2.  **Clone the Repository (if applicable):**
    ```bash
    git clone <your-repository-url>
    cd <repository-directory>
    ```

3.  **Create and Activate Virtual Environment:**
    ```bash
    # Create the environment
    python -m venv .venv

    # Activate (Windows CMD/PowerShell)
    .\.venv\Scripts\activate

    # Activate (Linux/macOS Bash)
    # source .venv/bin/activate
    ```

4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: A `requirements.txt` file is not automatically generated by this process, but you can create one using `pip freeze > requirements.txt` after installing the packages manually as done previously, or install them directly:)*
    ```bash
    pip install streamlit google-generativeai pypdf python-docx python-dotenv
    ```

5.  **Configure API Key:**
    *   Create a file named `.env` in the project root directory.
    *   Add your Google AI Studio API key to the file:
        ```dotenv
        GOOGLE_API_KEY="YOUR_ACTUAL_GOOGLE_API_KEY"
        ```
    *   Replace `"YOUR_ACTUAL_GOOGLE_API_KEY"` with your real key.
    *   The `.gitignore` file is configured to prevent accidentally committing your `.env` file.

## Running the Application

1.  Ensure your virtual environment is activated.
2.  Make sure your `GOOGLE_API_KEY` is set in the `.env` file.
3.  Run the Streamlit application from the project root directory:
    ```bash
    streamlit run app.py
    ```
4.  The application should open in your default web browser.

## How it Works

1.  The user uploads a document via the Streamlit interface.
2.  The application extracts the text content using `document_processor.py`.
3.  The token count for the extracted text is calculated using the Gemini API.
4.  If the token count is within the model's limit (1,000,000 tokens), the content is stored, and the token count is displayed.
5.  The user enters a query in the chat input.
6.  The application constructs a prompt containing recent chat history, the full document text, and the user's query.
7.  It checks if the *combined* prompt exceeds the token limit. If it does, it attempts to reduce the included chat history.
8.  If the prompt fits, it's sent to the `gemini-2.5-pro-exp-03-25` model.
9.  The model's response is displayed in the chat interface.
